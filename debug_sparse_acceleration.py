#!/usr/bin/env python3
"""
è°ƒè¯•2:4ç¨€ç–åŠ é€Ÿé—®é¢˜
"""

import torch
import time
import numpy as np
from diffusers import DDIMPipeline, DDIMScheduler

def check_sparse_support():
    """æ£€æŸ¥A100ç¨€ç–æ”¯æŒæƒ…å†µ"""
    print("ğŸ” æ£€æŸ¥A100ç¡¬ä»¶å’Œè½¯ä»¶ç¨€ç–æ”¯æŒ...")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"è®¾å¤‡: {torch.cuda.get_device_name()}")
    
    # æ£€æŸ¥A100ç‰¹å®šè¦æ±‚
    device_name = torch.cuda.get_device_name()
    if "A100" in device_name:
        print("âœ… A100ç¡¬ä»¶ï¼šæ”¯æŒ2:4ç»“æ„åŒ–ç¨€ç–")
    else:
        print(f"âš ï¸  éA100ç¡¬ä»¶({device_name})ï¼šå¯èƒ½ä¸æ”¯æŒç¡¬ä»¶ç¨€ç–åŠ é€Ÿ")
    
    # æ£€æŸ¥PyTorchç‰ˆæœ¬
    pytorch_version = torch.__version__
    major, minor = pytorch_version.split('.')[:2]
    if int(major) >= 2 or (int(major) == 1 and int(minor) >= 12):
        print("âœ… PyTorchç‰ˆæœ¬ï¼šæ”¯æŒA100ç¨€ç–åŠ é€Ÿ")
    else:
        print(f"âš ï¸  PyTorchç‰ˆæœ¬è¿‡ä½({pytorch_version})ï¼šå»ºè®®å‡çº§åˆ°1.12+")
    
    # å¯ç”¨A100ä¼˜åŒ–é…ç½®
    try:
        # å¯ç”¨ä¼˜åŒ–é…ç½®
        if hasattr(torch.backends.cuda, 'enable_flash_sdp'):
            torch.backends.cuda.enable_flash_sdp(True)
            print("âœ… Flash SDPï¼šå·²å¯ç”¨")
        
        if hasattr(torch.backends.cuda, 'allow_tf32'):
            torch.backends.cuda.allow_tf32 = True
            print("âœ… TF32ï¼šå·²å¯ç”¨")
            
        # æ£€æŸ¥Tensor Coreæ”¯æŒ
        if hasattr(torch.backends.cudnn, 'allow_tf32'):
            torch.backends.cudnn.allow_tf32 = True
            print("âœ… cuDNN TF32ï¼šå·²å¯ç”¨")
            
    except Exception as e:
        print(f"âš ï¸  ä¼˜åŒ–é…ç½®è®¾ç½®å¤±è´¥: {e}")
    
    # æ£€æŸ¥æ˜¯å¦æ”¯æŒ2:4ç¨€ç–
    device = torch.device("cuda")
    
    # æµ‹è¯•åŸºæœ¬çš„ç¨€ç–å¼ é‡æ”¯æŒ
    try:
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„2:4ç¨€ç–æ¨¡å¼
        dense = torch.randn(4, 4, device=device, dtype=torch.float16)  # ä½¿ç”¨FP16
        # åˆ›å»º2:4æ©ç  [1,1,0,0] æ¨¡å¼
        mask = torch.tensor([[1,1,0,0], [0,0,1,1], [1,0,1,0], [0,1,0,1]], 
                           dtype=torch.bool, device=device)
        sparse_tensor = dense * mask.float()
        
        print(f"âœ… FP16ç¨€ç–å¼ é‡æ“ä½œï¼šæ”¯æŒ")
        print(f"ç¨€ç–åº¦: {(sparse_tensor == 0).float().mean().item()*100:.1f}%")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰SparseSemiStructuredTensoræ”¯æŒï¼ˆPyTorch 2.0+ï¼‰
        if hasattr(torch, 'sparse'):
            if hasattr(torch.sparse, 'SparseSemiStructuredTensor'):
                print("âœ… SparseSemiStructuredTensorï¼šæ”¯æŒ")
                
                # æµ‹è¯•çœŸæ­£çš„ç¨€ç–å¼ é‡åˆ›å»º
                try:
                    # åˆ›å»º2:4ç¨€ç–å¼ é‡
                    dense_weights = torch.randn(128, 128, device=device, dtype=torch.float16)
                    sparse_weights = torch.sparse.SparseSemiStructuredTensor.from_dense(dense_weights)
                    print("âœ… 2:4ç¨€ç–å¼ é‡åˆ›å»ºï¼šæˆåŠŸ")
                except Exception as se:
                    print(f"âš ï¸  2:4ç¨€ç–å¼ é‡åˆ›å»ºå¤±è´¥: {se}")
            else:
                print("âŒ SparseSemiStructuredTensorï¼šä¸æ”¯æŒ")
        else:
            print("âŒ torch.sparseæ¨¡å—ï¼šä¸æ”¯æŒ")
            
    except Exception as e:
        print(f"âŒ ç¨€ç–å¼ é‡æµ‹è¯•å¤±è´¥: {e}")

def test_different_configs():
    """æµ‹è¯•ä¸åŒé…ç½®çš„æ€§èƒ½"""
    print("\nğŸ§ª æµ‹è¯•ä¸åŒé…ç½®çš„æ€§èƒ½...")
    
    dense_path = "/data/xay/MaskDM/pretrained/ddpm_ema_cifar10"
    pruned_path = "/data/xay/MaskDM/finetuned_results/masked_full_finetuned_v2/pruned/unet_ema_pruned.pth"
    model_dir = "/data/xay/MaskDM/finetuned_results/masked_full_finetuned_v2"
    
    configs = [
        {"dtype": torch.float32, "steps": 20, "name": "FP32_20æ­¥(åŸºå‡†)"},
        {"dtype": torch.float16, "steps": 20, "name": "FP16_20æ­¥(A100ç¨€ç–)"},
        {"dtype": torch.float16, "steps": 10, "name": "FP16_10æ­¥(å¿«é€Ÿæµ‹è¯•)"},
        {"dtype": torch.float16, "steps": 50, "name": "FP16_50æ­¥(å®Œæ•´æµ‹è¯•)"},
    ]
    
    results = {}
    
    for config in configs:
        print(f"\nğŸ”¸ æµ‹è¯•é…ç½®: {config['name']}")
        
        try:
            # åŠ è½½Denseæ¨¡å‹
            dense_pipeline = DDIMPipeline.from_pretrained(
                dense_path,
                torch_dtype=config["dtype"],
                use_safetensors=True
            )
            dense_pipeline = dense_pipeline.to('cuda')
            
            # åŠ è½½å‰ªææ¨¡å‹
            unet = torch.load(pruned_path, map_location='cpu')
            unet = unet.to('cuda', dtype=config["dtype"])
            
            pruned_pipeline = DDIMPipeline(
                unet=unet,
                scheduler=DDIMScheduler.from_pretrained(model_dir, subfolder="scheduler")
            )
            pruned_pipeline = pruned_pipeline.to('cuda')
            
            # ç®€å•æµ‹è¯•
            torch.cuda.synchronize()
            
            # Denseæµ‹è¯•
            start_time = time.time()
            with torch.no_grad():
                _ = dense_pipeline(
                    batch_size=1,
                    num_inference_steps=config["steps"],
                    generator=torch.Generator(device='cuda').manual_seed(42)
                ).images
            torch.cuda.synchronize()
            dense_time = time.time() - start_time
            
            # å‰ªææµ‹è¯•
            start_time = time.time()
            with torch.no_grad():
                _ = pruned_pipeline(
                    batch_size=1,
                    num_inference_steps=config["steps"],
                    generator=torch.Generator(device='cuda').manual_seed(42)
                ).images
            torch.cuda.synchronize()
            pruned_time = time.time() - start_time
            
            speedup = dense_time / pruned_time
            results[config['name']] = {
                'dense': dense_time,
                'pruned': pruned_time,
                'speedup': speedup
            }
            
            print(f"  Dense: {dense_time:.3f}s")
            print(f"  å‰ªæ: {pruned_time:.3f}s")
            print(f"  åŠ é€Ÿæ¯”: {speedup:.2f}x")
            
            # æ¸…ç†å†…å­˜
            del dense_pipeline, pruned_pipeline, unet
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
            results[config['name']] = None
    
    return results

def test_batch_sizes():
    """æµ‹è¯•ä¸åŒæ‰¹é‡å¤§å°çš„å½±å“"""
    print("\nğŸ”¢ æµ‹è¯•ä¸åŒæ‰¹é‡å¤§å°...")
    
    dense_path = "/data/xay/MaskDM/pretrained/ddpm_ema_cifar10"
    pruned_path = "/data/xay/MaskDM/finetuned_results/masked_full_finetuned_v2/pruned/unet_ema_pruned.pth"
    model_dir = "/data/xay/MaskDM/finetuned_results/masked_full_finetuned_v2"
    
    batch_sizes = [1, 2, 4, 8]
    
    try:
        # åŠ è½½æ¨¡å‹ (ä½¿ç”¨FP16ï¼Œ20æ­¥)
        dense_pipeline = DDIMPipeline.from_pretrained(
            dense_path,
            torch_dtype=torch.float16,
            use_safetensors=True
        )
        dense_pipeline = dense_pipeline.to('cuda')
        
        unet = torch.load(pruned_path, map_location='cpu')
        unet = unet.to('cuda', dtype=torch.float16)
        
        pruned_pipeline = DDIMPipeline(
            unet=unet,
            scheduler=DDIMScheduler.from_pretrained(model_dir, subfolder="scheduler")
        )
        pruned_pipeline = pruned_pipeline.to('cuda')
        
        for batch_size in batch_sizes:
            print(f"\nğŸ”¸ æ‰¹é‡å¤§å°: {batch_size}")
            
            try:
                # Denseæµ‹è¯•
                torch.cuda.synchronize()
                start_time = time.time()
                with torch.no_grad():
                    _ = dense_pipeline(
                        batch_size=batch_size,
                        num_inference_steps=20,
                        generator=torch.Generator(device='cuda').manual_seed(42)
                    ).images
                torch.cuda.synchronize()
                dense_time = time.time() - start_time
                
                # å‰ªææµ‹è¯•
                torch.cuda.synchronize()
                start_time = time.time()
                with torch.no_grad():
                    _ = pruned_pipeline(
                        batch_size=batch_size,
                        num_inference_steps=20,
                        generator=torch.Generator(device='cuda').manual_seed(42)
                    ).images
                torch.cuda.synchronize()
                pruned_time = time.time() - start_time
                
                speedup = dense_time / pruned_time
                throughput_dense = batch_size / dense_time
                throughput_pruned = batch_size / pruned_time
                
                print(f"  Dense: {dense_time:.3f}s ({throughput_dense:.2f} imgs/s)")
                print(f"  å‰ªæ: {pruned_time:.3f}s ({throughput_pruned:.2f} imgs/s)")
                print(f"  åŠ é€Ÿæ¯”: {speedup:.2f}x")
                
            except Exception as e:
                print(f"  âŒ æ‰¹é‡{batch_size}æµ‹è¯•å¤±è´¥: {e}")
        
        # æ¸…ç†
        del dense_pipeline, pruned_pipeline, unet
        torch.cuda.empty_cache()
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡æµ‹è¯•å¤±è´¥: {e}")

def enable_a100_optimizations():
    """å¯ç”¨A100ä¼˜åŒ–é…ç½®"""
    try:
        # å¯ç”¨æ‰€æœ‰A100ç›¸å…³ä¼˜åŒ–
        if hasattr(torch.backends.cuda, 'enable_flash_sdp'):
            torch.backends.cuda.enable_flash_sdp(True)
        if hasattr(torch.backends.cuda, 'allow_tf32'):
            torch.backends.cuda.allow_tf32 = True
        if hasattr(torch.backends.cudnn, 'allow_tf32'):
            torch.backends.cudnn.allow_tf32 = True
        print("âœ… A100ä¼˜åŒ–é…ç½®å·²å¯ç”¨")
    except Exception as e:
        print(f"âš ï¸  ä¼˜åŒ–é…ç½®å¯ç”¨å¤±è´¥: {e}")

def main():
    print("ğŸ”§ A100ç¨€ç–åŠ é€Ÿè°ƒè¯•å·¥å…·")
    print("="*50)
    
    # 0. å¯ç”¨A100ä¼˜åŒ–
    enable_a100_optimizations()
    
    # 1. æ£€æŸ¥åŸºæœ¬æ”¯æŒ
    check_sparse_support()
    
    # 2. æµ‹è¯•ä¸åŒé…ç½®
    print("\n" + "="*50)
    config_results = test_different_configs()
    
    # 3. æµ‹è¯•æ‰¹é‡å¤§å°
    print("\n" + "="*50)
    test_batch_sizes()
    
    # 4. æ€»ç»“
    print("\n" + "="*50)
    print("ğŸ“Š æ€»ç»“åˆ†æ")
    print("="*50)
    
    if config_results:
        best_config = None
        best_speedup = 0
        
        for config_name, result in config_results.items():
            if result and result['speedup'] > best_speedup:
                best_speedup = result['speedup']
                best_config = config_name
        
        if best_config:
            print(f"âœ… æœ€ä½³é…ç½®: {best_config}")
            print(f"âœ… æœ€ä½³åŠ é€Ÿæ¯”: {best_speedup:.2f}x")
        else:
            print("âŒ æœªå‘ç°æ˜æ˜¾åŠ é€Ÿ")
    
    print(f"\nğŸ’¡ å¯èƒ½çš„æ”¹è¿›å»ºè®®:")
    print(f"  1. å°è¯•ä½¿ç”¨float16è€Œä¸æ˜¯float32")
    print(f"  2. å‡å°‘é‡‡æ ·æ­¥æ•°ï¼ˆ20æ­¥è€Œä¸æ˜¯50æ­¥ï¼‰")
    print(f"  3. å¢åŠ æ‰¹é‡å¤§å°ï¼ˆå¦‚æœå†…å­˜å…è®¸ï¼‰")
    print(f"  4. æ£€æŸ¥PyTorchç‰ˆæœ¬æ˜¯å¦æ”¯æŒA100ç¨€ç–ä¼˜åŒ–")
    print(f"  5. ç¡®è®¤æ¨¡å‹çš„ç¨€ç–æ¨¡å¼æ˜¯å¦æ­£ç¡®å®ç°")

if __name__ == "__main__":
    main()
