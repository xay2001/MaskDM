#!/usr/bin/env python3
"""
æ£€æŸ¥çœŸæ­£å‰ªæåæ¨¡å‹çš„å‚æ•°é‡
"""

import torch
import os

def count_parameters(model):
    """è®¡ç®—æ¨¡å‹å‚æ•°é‡"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return total_params, trainable_params

def format_params(num_params):
    """æ ¼å¼åŒ–å‚æ•°æ•°é‡"""
    if num_params >= 1e9:
        return f"{num_params/1e9:.2f}B"
    elif num_params >= 1e6:
        return f"{num_params/1e6:.2f}M"
    elif num_params >= 1e3:
        return f"{num_params/1e3:.2f}K"
    else:
        return str(num_params)

def main():
    # æ£€æŸ¥åŸå§‹çš„å¸¦maskæ¨¡å‹
    mask_model_path = "/data/xay/MaskDM/Maskpro/train_result/config2_standard/lr1.0_epoch2000_logits5.0_size20000_diffusion/checkpoint"
    
    # æ£€æŸ¥çœŸæ­£å‰ªæåçš„æ¨¡å‹
    pruned_model_path = "/data/xay/MaskDM/finetuned_results/masked_full_finetuned_v2/pruned/unet_ema_pruned.pth"
    
    print("ğŸ” æ£€æŸ¥æ¨¡å‹å‚æ•°é‡å¯¹æ¯”...")
    
    if os.path.exists(pruned_model_path):
        print(f"\nğŸ“Š çœŸæ­£å‰ªæåçš„æ¨¡å‹:")
        print(f"è·¯å¾„: {pruned_model_path}")
        
        try:
            # åŠ è½½å‰ªæåçš„æ¨¡å‹
            pruned_model = torch.load(pruned_model_path, map_location='cpu')
            total_params, trainable_params = count_parameters(pruned_model)
            
            print(f"æ€»å‚æ•°é‡: {format_params(total_params)} ({total_params:,})")
            print(f"å¯è®­ç»ƒå‚æ•°: {format_params(trainable_params)} ({trainable_params:,})")
            
            # æ˜¾ç¤ºä¸€äº›å±‚çš„ä¿¡æ¯
            print(f"\nğŸ—ï¸ æ¨¡å‹ç±»å‹: {type(pruned_model).__name__}")
            
            # è®¡ç®—å‰ªææ¯”ä¾‹ï¼ˆå‡è®¾åŸå§‹æ¨¡å‹æ˜¯35.75Må‚æ•°ï¼‰
            original_params = 35746307
            pruning_ratio = (original_params - total_params) / original_params * 100
            print(f"ğŸ“‰ å‰ªææ¯”ä¾‹: {pruning_ratio:.2f}% (ç›¸æ¯”åŸå§‹æ¨¡å‹)")
            
        except Exception as e:
            print(f"âŒ åŠ è½½å‰ªææ¨¡å‹å¤±è´¥: {e}")
    else:
        print(f"âŒ å‰ªææ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {pruned_model_path}")
    
    # ä¹Ÿæ£€æŸ¥ä¸€ä¸‹å…¶ä»–å¯èƒ½çš„å‰ªææ¨¡å‹
    other_pruned_files = [
        "/data/xay/MaskDM/finetuned_results/masked_full_finetuned_v2/pruned/unet_pruned.pth",
        "/data/xay/MaskDM/finetuned_results/masked_full_finetuned_v2/pruned/unet_ema_pruned-100000.pth"
    ]
    
    for pruned_file in other_pruned_files:
        if os.path.exists(pruned_file):
            print(f"\nğŸ“Š æ£€æŸ¥æ–‡ä»¶: {os.path.basename(pruned_file)}")
            try:
                model = torch.load(pruned_file, map_location='cpu')
                total_params, _ = count_parameters(model)
                print(f"å‚æ•°é‡: {format_params(total_params)} ({total_params:,})")
            except Exception as e:
                print(f"âŒ åŠ è½½å¤±è´¥: {e}")

if __name__ == "__main__":
    main()


