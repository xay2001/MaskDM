#!/usr/bin/env python3
"""
å¿«é€Ÿæ¨ç†é€Ÿåº¦æµ‹è¯• - Dense vs 2:4å‰ªææ¨¡å‹å¯¹æ¯”
å‚è€ƒSparseDMæµ‹è¯•æ–¹æ³•ï¼šå®Œæ•´æ‰©æ•£é‡‡æ ·è¿‡ç¨‹è¯„ä¼°
"""

import torch
import time
import numpy as np
from diffusers import DDIMPipeline, DDIMScheduler
import gc

# å¯ç”¨A100ä¼˜åŒ–é…ç½®
try:
    if hasattr(torch.backends.cuda, 'enable_flash_sdp'):
        torch.backends.cuda.enable_flash_sdp(True)
    if hasattr(torch.backends.cuda, 'allow_tf32'):
        torch.backends.cuda.allow_tf32 = True
    if hasattr(torch.backends.cudnn, 'allow_tf32'):
        torch.backends.cudnn.allow_tf32 = True
except Exception:
    pass  # é™é»˜å¤±è´¥ï¼Œä¸å½±å“æµ‹è¯•

def get_gpu_memory():
    """è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ(GB)"""
    if torch.cuda.is_available():
        return torch.cuda.memory_allocated() / 1024**3
    return 0

def gpu_warmup(pipeline, steps=3):
    """GPUé¢„çƒ­ï¼Œå‚è€ƒSparseDMæ–¹æ³•"""
    print("ğŸ”¥ GPUé¢„çƒ­ä¸­...")
    with torch.no_grad():
        for i in range(steps):
            _ = pipeline(
                batch_size=1,
                num_inference_steps=10,
                generator=torch.Generator(device='cuda').manual_seed(42+i)
            ).images
    torch.cuda.synchronize()
    print("âœ… GPUé¢„çƒ­å®Œæˆ")

def test_dense_model():
    """æµ‹è¯•åŸå§‹Denseæ¨¡å‹ - å‚è€ƒSparseDMæµ‹è¯•æ–¹æ³•"""
    print("ğŸ“‚ åŠ è½½åŸå§‹Denseæ¨¡å‹...")
    try:
        dense_path = "/data/xay/MaskDM/pretrained/ddpm_ema_cifar10"
        
        pipeline = DDIMPipeline.from_pretrained(
            dense_path,
            torch_dtype=torch.float16,  # ä½¿ç”¨FP16å¯ç”¨A100ç¨€ç–åŠ é€Ÿ
            use_safetensors=True
        )
        pipeline.scheduler.skip_type = "uniform"
        pipeline = pipeline.to('cuda')
        
        # æ£€æŸ¥å‚æ•°é‡
        total_params = sum(p.numel() for p in pipeline.unet.parameters())
        print(f"âœ… Denseæ¨¡å‹åŠ è½½æˆåŠŸï¼å‚æ•°é‡: {total_params/1e6:.2f}M")
        
        # GPUé¢„çƒ­
        gpu_warmup(pipeline)
        
        # å¤šè½®æµ‹è¯•ä»¥è·å¾—ç¨³å®šç»“æœ
        print("âš¡ Denseæ¨¡å‹æ¨ç†æµ‹è¯•...")
        times = []
        memory_usage = []
        
        # ä½¿ç”¨20æ­¥é‡‡æ ·ä»¥æ›´å¥½å±•ç¤ºA100 FP16ç¨€ç–åŠ é€Ÿæ•ˆæœ
        num_runs = 5
        steps = 20
        
        for i in range(num_runs):
            torch.cuda.empty_cache()
            gc.collect()
            
            start_memory = get_gpu_memory()
            
            with torch.no_grad():
                torch.cuda.synchronize()
                start_time = time.time()
                
                images = pipeline(
                    batch_size=1,
                    num_inference_steps=steps,
                    generator=torch.Generator(device='cuda').manual_seed(42+i)
                ).images
                
                torch.cuda.synchronize()
                end_time = time.time()
            
            end_memory = get_gpu_memory()
            
            inference_time = end_time - start_time
            times.append(inference_time)
            memory_usage.append(end_memory - start_memory)
            
            print(f"  ç¬¬{i+1}è½®: {inference_time:.3f}ç§’, å†…å­˜: {end_memory-start_memory:.3f}GB")
        
        # ç»Ÿè®¡ç»“æœ
        times = np.array(times[1:])  # å»æ‰ç¬¬ä¸€æ¬¡é¢„çƒ­
        avg_time = np.mean(times)
        std_time = np.std(times)
        avg_memory = np.mean(memory_usage[1:])
        
        print(f"ğŸ“Š Denseæ¨¡å‹ç»Ÿè®¡ç»“æœ:")
        print(f"  å¹³å‡æ—¶é—´: {avg_time:.3f} Â± {std_time:.3f} ç§’")
        print(f"  æœ€å¿«æ—¶é—´: {np.min(times):.3f} ç§’")
        print(f"  æœ€æ…¢æ—¶é—´: {np.max(times):.3f} ç§’")
        print(f"  å¹³å‡å†…å­˜: {avg_memory:.3f} GB")
        print(f"  ååé‡: {1/avg_time:.2f} images/sec")
        
        # æ¸…ç†å†…å­˜
        del pipeline
        torch.cuda.empty_cache()
        gc.collect()
        
        return avg_time
        
    except Exception as e:
        print(f"âŒ Denseæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return None

def test_pruned_model():
    """æµ‹è¯•2:4å‰ªææ¨¡å‹ - å‚è€ƒSparseDMæµ‹è¯•æ–¹æ³•"""
    print("ğŸ“‚ åŠ è½½2:4å‰ªææ¨¡å‹...")
    try:
        # åŠ è½½å‰ªæåçš„UNet
        pruned_path = "/data/xay/MaskDM/finetuned_results/masked_full_finetuned_v2/pruned/unet_ema_pruned.pth"
        model_dir = "/data/xay/MaskDM/finetuned_results/masked_full_finetuned_v2"
        
        unet = torch.load(pruned_path, map_location='cpu')
        unet = unet.to('cuda', dtype=torch.float16)  # ä½¿ç”¨FP16å¯ç”¨A100ç¨€ç–åŠ é€Ÿ
        
        pipeline = DDIMPipeline(
            unet=unet,
            scheduler=DDIMScheduler.from_pretrained(model_dir, subfolder="scheduler")
        )
        pipeline = pipeline.to('cuda')
        
        # æ£€æŸ¥ç¨€ç–åº¦
        total_params = sum(p.numel() for p in unet.parameters())
        zero_params = sum(torch.sum(p==0).item() for p in unet.parameters())
        sparsity = zero_params / total_params * 100
        effective_params = total_params - zero_params
        print(f"âœ… å‰ªææ¨¡å‹åŠ è½½æˆåŠŸï¼ç¨€ç–åº¦: {sparsity:.2f}%ï¼Œæœ‰æ•ˆå‚æ•°: {effective_params/1e6:.2f}M")
        
        # GPUé¢„çƒ­
        gpu_warmup(pipeline)
        
        # å¤šè½®æµ‹è¯•ä»¥è·å¾—ç¨³å®šç»“æœ
        print("âš¡ å‰ªææ¨¡å‹æ¨ç†æµ‹è¯•...")
        times = []
        memory_usage = []
        
        # ä½¿ç”¨20æ­¥é‡‡æ ·ä»¥æ›´å¥½å±•ç¤ºA100 FP16ç¨€ç–åŠ é€Ÿæ•ˆæœ
        num_runs = 5
        steps = 20
        
        for i in range(num_runs):
            torch.cuda.empty_cache()
            gc.collect()
            
            start_memory = get_gpu_memory()
            
            with torch.no_grad():
                torch.cuda.synchronize()
                start_time = time.time()
                
                images = pipeline(
                    batch_size=1,
                    num_inference_steps=steps,
                    generator=torch.Generator(device='cuda').manual_seed(42+i)
                ).images
                
                torch.cuda.synchronize()
                end_time = time.time()
            
            end_memory = get_gpu_memory()
            
            inference_time = end_time - start_time
            times.append(inference_time)
            memory_usage.append(end_memory - start_memory)
            
            print(f"  ç¬¬{i+1}è½®: {inference_time:.3f}ç§’, å†…å­˜: {end_memory-start_memory:.3f}GB")
        
        # ç»Ÿè®¡ç»“æœ
        times = np.array(times[1:])  # å»æ‰ç¬¬ä¸€æ¬¡é¢„çƒ­
        avg_time = np.mean(times)
        std_time = np.std(times)
        avg_memory = np.mean(memory_usage[1:])
        
        print(f"ğŸ“Š å‰ªææ¨¡å‹ç»Ÿè®¡ç»“æœ:")
        print(f"  å¹³å‡æ—¶é—´: {avg_time:.3f} Â± {std_time:.3f} ç§’")
        print(f"  æœ€å¿«æ—¶é—´: {np.min(times):.3f} ç§’")
        print(f"  æœ€æ…¢æ—¶é—´: {np.max(times):.3f} ç§’")
        print(f"  å¹³å‡å†…å­˜: {avg_memory:.3f} GB")
        print(f"  ååé‡: {1/avg_time:.2f} images/sec")
        
        # æ¸…ç†å†…å­˜
        del pipeline
        torch.cuda.empty_cache()
        gc.collect()
        
        return avg_time
        
    except Exception as e:
        print(f"âŒ å‰ªææ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return None

def quick_comparison_test():
    """å¿«é€Ÿå¯¹æ¯”æµ‹è¯• - å‚è€ƒSparseDMæ ‡å‡†æµ‹è¯•æ–¹æ³•"""
    print("ğŸš€ A100 FP16ç¨€ç–åŠ é€Ÿæµ‹è¯•")
    print(f"ğŸ–¥ï¸  è®¾å¤‡: {torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU'}")
    print(f"ğŸ”§ CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"ğŸ“¦ PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"ğŸ§  æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB")
    print("="*60)
    print("ğŸ¯ A100ç¨€ç–åŠ é€Ÿä¼˜åŒ–é…ç½®:")
    print("  - æ•°æ®ç±»å‹: FP16 (å¯ç”¨Tensor Coreç¨€ç–åŠ é€Ÿ)")
    print("  - é‡‡æ ·æ­¥æ•°: 20æ­¥ (ä¼˜åŒ–ç¨€ç–åŠ é€Ÿæ•ˆæœ)")
    print("  - æµ‹è¯•è½®æ¬¡: 5è½® (å»é™¤ç¬¬1è½®é¢„çƒ­)")
    print("  - æ‰¹é‡å¤§å°: 1")
    print("  - ç¨€ç–æ¨¡å¼: 2:4ç»“æ„åŒ–ç¨€ç–")
    print("="*60)
    
    # æµ‹è¯•Denseæ¨¡å‹
    print("\nğŸ”¸ æµ‹è¯•1: åŸå§‹Denseæ¨¡å‹")
    dense_time = test_dense_model()
    
    print("\n" + "-"*60)
    
    # æµ‹è¯•å‰ªææ¨¡å‹
    print("\nğŸ”¹ æµ‹è¯•2: 2:4å‰ªææ¨¡å‹")
    pruned_time = test_pruned_model()
    
    # å¯¹æ¯”ç»“æœ
    print("\n" + "="*60)
    print("ğŸ“Š æœ€ç»ˆå¯¹æ¯”ç»“æœ")
    print("="*60)
    
    if dense_time is not None and pruned_time is not None:
        speedup = dense_time / pruned_time
        time_saved = dense_time - pruned_time
        percentage_improvement = (time_saved / dense_time) * 100
        
        print(f"ğŸ”¸ Denseæ¨¡å‹å¹³å‡æ—¶é—´:     {dense_time:.3f} ç§’")
        print(f"ğŸ”¹ 2:4å‰ªææ¨¡å‹å¹³å‡æ—¶é—´:  {pruned_time:.3f} ç§’")
        print(f"âš¡ åŠ é€Ÿæ¯”:               {speedup:.2f}x")
        print(f"â±ï¸  æ—¶é—´èŠ‚çœ:             {time_saved:.3f} ç§’ ({percentage_improvement:.1f}%)")
        print(f"ğŸš€ æ¯ç§’ç”Ÿæˆå›¾ç‰‡æ•°æå‡:    {(1/pruned_time)/(1/dense_time):.2f}x")
        
        print(f"\nğŸ” ç»“æœåˆ†æ:")
        if speedup >= 3.0:
            print(f"ğŸ‰ åœ¨A100ä¸Šè·å¾—äº†æ˜¾è‘—çš„ç¡¬ä»¶åŠ é€Ÿï¼")
            print(f"ğŸ’¡ 2:4ç»“æ„åŒ–å‰ªæå……åˆ†åˆ©ç”¨äº†A100çš„Tensor CoreåŠ é€Ÿ")
        elif speedup >= 1.5:
            print(f"âœ… è·å¾—äº†è‰¯å¥½çš„åŠ é€Ÿæ•ˆæœ")
            print(f"ğŸ’¡ ç¡¬ä»¶åŠ é€Ÿæ­£å¸¸å·¥ä½œ")
        elif speedup >= 1.1:
            print(f"âš ï¸  åŠ é€Ÿæ•ˆæœè¾ƒè½»å¾®")
            print(f"ğŸ”§ å¯èƒ½éœ€è¦æ£€æŸ¥CUDAé…ç½®æˆ–æ¨¡å‹ä¼˜åŒ–")
        else:
            print(f"âŒ åŠ é€Ÿæ•ˆæœä¸æ˜æ˜¾")
            print(f"ğŸ”§ å»ºè®®æ£€æŸ¥ç¡¬ä»¶æ”¯æŒå’Œæ¨¡å‹å®ç°")
        
        print(f"\nğŸ“Š ä¸SparseDMå¯¹æ¯”:")
        print(f"  - SparseDMåŠ é€Ÿæ¯”: ~1.2x")
        print(f"  - æœ¬æµ‹è¯•åŠ é€Ÿæ¯”: {speedup:.2f}x")
        if speedup > 1.2:
            print(f"  âœ… è¶…è¶Šäº†SparseDMçš„æ€§èƒ½ï¼")
        else:
            print(f"  ğŸ“ ä¸SparseDMç»“æœç›¸è¿‘")
            
    elif dense_time is not None:
        print(f"ğŸ”¸ Denseæ¨¡å‹æµ‹è¯•æˆåŠŸ: {dense_time:.3f}ç§’")
        print(f"âŒ å‰ªææ¨¡å‹æµ‹è¯•å¤±è´¥")
    elif pruned_time is not None:
        print(f"âŒ Denseæ¨¡å‹æµ‹è¯•å¤±è´¥")
        print(f"ğŸ”¹ å‰ªææ¨¡å‹æµ‹è¯•æˆåŠŸ: {pruned_time:.3f}ç§’")
    else:
        print(f"âŒ ä¸¤ä¸ªæ¨¡å‹æµ‹è¯•éƒ½å¤±è´¥äº†")
    
    print(f"\nğŸ“„ æµ‹è¯•æ–¹æ³•è¯´æ˜:")
    print(f"  - æœ¬æµ‹è¯•é‡‡ç”¨ä¸SparseDMç›¸åŒçš„è¯„ä¼°æ–¹æ³•")
    print(f"  - æµ‹è¯•å®Œæ•´çš„æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ï¼Œè€Œéå•æ­¥æ¨ç†")
    print(f"  - ç»“æœæ›´èƒ½åæ˜ å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½æå‡")
    print(f"  - å¦‚éœ€æ›´è¯¦ç»†åˆ†æï¼Œè¯·è¿è¡Œ speed_benchmark.py")

if __name__ == "__main__":
    quick_comparison_test()
