#!/bin/bash

# å®Œæ•´ç‰ˆmaskçº¦æŸå¾®è°ƒè„šæœ¬ - å®Œå…¨åŒ¹é…magnitudeå‚æ•°

echo "ğŸš€ å¼€å§‹å®Œæ•´ç‰ˆmaskçº¦æŸå¾®è°ƒï¼ˆåŒ¹é…magnitudeè®¾ç½®ï¼‰..."

# è®¾ç½®è·¯å¾„
PRUNED_MODEL_PATH="/data/xay/MaskDM/Maskpro/train_result/config2_standard/lr1.0_epoch2000_logits5.0_size20000_diffusion/checkpoint"
OUTPUT_DIR="/data/xay/MaskDM/finetuned_results/masked_full_finetuned_v2"

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p $OUTPUT_DIR

echo "ğŸ“ å‰ªææ¨¡å‹è·¯å¾„: $PRUNED_MODEL_PATH"
echo "ğŸ“ è¾“å‡ºç›®å½•: $OUTPUT_DIR"

# å®Œå…¨åŒ¹é…magnitudeè„šæœ¬çš„å‚æ•°
python ddpm_train_simple_masked.py \
    --dataset cifar10 \
    --model_path $PRUNED_MODEL_PATH \
    --resolution 32 \
    --output_dir $OUTPUT_DIR \
    --train_batch_size 128 \
    --num_iters 100000 \
    --gradient_accumulation_steps 1 \
    --learning_rate 2e-4 \
    --lr_warmup_steps 0 \
    --save_model_steps 1000 \
    --dataloader_num_workers 8 \
    --adam_weight_decay 0.00 \
    --ema_max_decay 0.9999 \
    --dropout 0.1 \
    --use_ema \
    --logger wandb \
    --overwrite_output_dir

echo "âœ… å®Œæ•´å¾®è°ƒå®Œæˆï¼"
echo "ğŸ“Š è¾“å‡ºç›®å½•: $OUTPUT_DIR"
echo "ğŸ–¼ï¸  ç”Ÿæˆæ ·æœ¬: $OUTPUT_DIR/vis/"
echo "ğŸ’¾ æ¨¡å‹æ£€æŸ¥ç‚¹: $OUTPUT_DIR/pruned/"
echo ""
echo "ğŸ“ˆ è®­ç»ƒç»Ÿè®¡:"
echo "   - æ€»è¿­ä»£æ¬¡æ•°: 100,000"
echo "   - ç­‰æ•ˆepochæ•°: ~256"
echo "   - Batch size: 128"
echo "   - å­¦ä¹ ç‡: 2e-4"
